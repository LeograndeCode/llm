{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transformers, a fundamental step is to convert the input text into a sequence of tokens. Tokenizers are used for this purpose. Different tokenization techniques can be used (e.g., Byte-Pair Encoding). \n",
    "\n",
    "These tokenizers need to be trained on some corpus (e.g., to figure out what the most common words are). However, the Hugging Face library provides pre-trained tokenizers that can be used out of the box.\n",
    "\n",
    "Generally, each model has its own tokenizer. For example, the `BertTokenizer` is used for BERT models, and the `GPT2Tokenizer` is used for GPT-2 models. \n",
    "\n",
    "\n",
    "Since we will be using T5 for this exercise, we should be using the `T5Tokenizer` class. However, HuggingFace provides a common `AutoTokenizer` class that can be used to load the appropriate tokenizer for a given model  (do note, however, that the returned class will be the \"correct\" one!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgiobergia/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"google-t5/t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding/decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization can be carried out by passing a string that we want to tokenize. The tokenizer implements the `__call__` method, so we can call the tokenizer directly, as follows.\n",
    "\n",
    "Note that the output is a dictionary, which generally has the following keys:\n",
    "\n",
    "- `input_ids`: The tokenized input text (a list of token IDs by default). \n",
    "- `attention_mask`: A mask that indicates which elements in the input text are tokens and which are padding tokens. For now, we can ignore this (there is no padding). It will instead become useful when we encode batches of sentences of different lengths at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21820, 6, 48, 19, 3, 9, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"hello, this is a sentence!\"\n",
    "tokens = tokenizer(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the encoding operation (i.e., going from token IDs to strings) by using the `decode` method of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, this is a sentence!</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an extra part at the end of the string, which is the special token `</s>`. This token is used to indicate the end of the input text (EOS). This token is automatically added by the tokenizer when encoding the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn what the mapping between tokens and token IDs is, we can get the tokenizer's vocabulary (`.get_vocab()`), which provides the mapping between tokens and respective IDs. \n",
    "\n",
    "For convenience, we build also a reverse vocabulary (i.e., from IDs to tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁Ridge': 13068,\n",
       " '▁Pride': 24252,\n",
       " '▁empfehlen': 13239,\n",
       " '▁Leicht': 29774,\n",
       " '▁Phil': 8188,\n",
       " '▁republic': 20237,\n",
       " '▁Gin': 15079,\n",
       " '▁spate': 14911,\n",
       " '▁Ban': 5185,\n",
       " '▁SPD': 22822}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "reverse_vocab = { v: k for k, v in vocabulary.items() }\n",
    "\n",
    "vocab_keys = list(vocabulary.keys())\n",
    "\n",
    "\n",
    "random.shuffle(vocab_keys)\n",
    "\n",
    "# Show 10 random words from the vocabulary\n",
    "{ k: vocabulary[k] for k in vocab_keys[:10] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 32100\n"
     ]
    }
   ],
   "source": [
    "print(\"Total vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 32100. For T5, that's 32000 tokens + 100 special tokens (<extra_id_0>, <extra_id_1>, ..., <extra_id_99>) -- used for the tasks that T5 was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the token id for the special token `</s>` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[\"</s>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, note that our `tokens` has a 1 showing up at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21820, 6, 48, 19, 3, 9, 7142, 55, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include special tokens inside of the strings themselves. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [21820, 55, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello!</s></s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have 2 `</s>` tokens (the ones we specified), plus an additional one that was added by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting token IDs directly, we may look at the tokens being produced, directly. We use the `tokenize()` method in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁hello', ',', '▁this', '▁is', '▁', 'a', '▁sentence', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's up with those `_`? They simply represent words that are starting after spaces. This helps us understand whether a token is being used at the beginning of a sentence, or if it's in the middle of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁hello', '▁', ',', 'world']\n",
      "['▁hello', '▁', ',', '▁world']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"hello    ,world\"))\n",
    "print(tokenizer.tokenize(\"hello    , world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, `_hello` is the token for the word \"hello\" at the beginning of the sentence. However, the word \"world\" is mapped to two different tokens, depending on whether there is a space before the word or not. \n",
    "\n",
    "Notice also how multiple spaces are compacted into a single one!\n",
    "\n",
    "These are all tokenizer-specific details. The tokenizer is responsible for deciding how to tokenize the input text. You may observe different behaviors for different tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "\n",
    "Each model typically has its own special tokens. Some are necessary for the training process, while others can be beneficial at inference time.\n",
    "\n",
    "Special attributes are available in the tokenizer class to access these special tokens. Some examples are:\n",
    "\n",
    "- `pad_token` is the token used for padding (as discussed later),\n",
    "- `bos_token` and `eos_token` tokens are used to indicate the beginning and end of the input text, respectively,\n",
    "- `mask_token` is used for masking tokens during training (e.g., for the masked LM task, with BERT),\n",
    "- `sep_token` is used to separate sentences in the input text (e.g., next sentence prediction, with BERT),\n",
    "- `cls_token` is used to indicate the beginning of the input text (e.g., for classification tasks, with BERT),\n",
    "- `unk_token` is used to indicate unknown tokens (i.e., tokens that are not in the vocabulary).\n",
    "\n",
    "Of course, not all tokenizers will use all tokens. So those attributes will be set to None, if not used.\n",
    "\n",
    "For instance, T5 has EOS and PAD tokens, but no BOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '<pad>', None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.pad_token, tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_id` suffix is used to indicate the corresponding token ID (None if not applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch encoding/decoding\n",
    "\n",
    "In general (especially at training time) we will want to encode multiple sentences at once (e.g., an entire batch of sentences).\n",
    "\n",
    "We can pass a list of sentences to be encoded to the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 19, 8, 166, 7142, 1]\n",
      "[1446, 6, 48, 19, 8, 511, 5932, 55, 1]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"this is the first sentence\",\n",
    "    \"instead, this is the second sequence!\"\n",
    "]\n",
    "tokens = tokenizer(sentences)\n",
    "\n",
    "for tok in tokens[\"input_ids\"]:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, sentences of different lengths have a different number of tokens! However, tensors (that will be used by the model) need to have the same number of elements along each dimension. \n",
    "\n",
    "To do this, we can use padding: all sentences will be padded to the length of the longest sentence in the batch. This is done by adding `pad` tokens (`<pad>`, for T5). \n",
    "\n",
    "However, since the pad tokens are not part of the input text, we need to let the model know that it should not pay attention to them. That's what the `attention_mask` is for! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 19, 8, 166, 7142, 1, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1446, 6, 48, 19, 8, 511, 5932, 55, 1] [1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sentences, padding=True)\n",
    "\n",
    "for tok, att in zip(tokens[\"input_ids\"], tokens[\"attention_mask\"]):\n",
    "    print(tok, att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence is padded to the same length as the second sentence, with 0's (remember, the ID for `<pad>`!). \n",
    "\n",
    "The attention mask for the first sentence also contains 0's for the padding tokens: the model will ignore them when processing the input text.\n",
    "\n",
    "Since now all sentences have the same length, we can stack them into a single tensor. Luckily, the tokenizer can already do this for us, we just need to ask. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  48,   19,    8,  166, 7142,    1,    0,    0,    0],\n",
      "        [1446,    6,   48,   19,    8,  511, 5932,   55,    1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# note: we pass return_tensors=\"pt\" to get PyTorch tensors\n",
    "# (the library also supports TensorFlow tensors, but we\n",
    "# don't care about them!)\n",
    "tokens = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "print(tokens[\"input_ids\"])\n",
    "print(tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and finally load our T5 model. We once again use a pretrained version available on HuggingFace. \n",
    "\n",
    "In general, we could use the `AutoModel` class for the loading, of the model. However, that version of the model does not include the specific heads for the tasks that T5 was trained on.\n",
    "\n",
    "The transformers library can make our life easier by defining a family of `AutoModel...` classes. \n",
    "\n",
    "For instance, the following are some commonly adopted classes:\n",
    "- `AutoModel`: the base class for all models,\n",
    "- `AutoModelForSequenceClassification`: a model for sequence classification tasks. It consists of a base model plus a classification head (linear layer + softmax). Note that, generally, the classification head is initialized randomly, and it needs to be trained on the specific task (but the library will let you know with a warning),\n",
    "- `AutoModelForCaualLM`: a model for causal language modeling tasks (e.g., GPT-2), where we generate the output tokens one by one,\n",
    "- `AutoModelForMaskedLM`: a model for masked language modeling tasks (e.g., BERT), where we predict the masked tokens in the input text,\n",
    "- `AutoModelForTokenClassification`: a model for token classification tasks (e.g., NER), where we classify each token in the input text.\n",
    "- `AutoModelForSeq2SeqLM`: a model for sequence-to-sequence tasks (e.g., T5), where we generate the output tokens one by one in an autoregressive manner, conditioned on the input sequence.\n",
    "\n",
    "In our specific case, we will use the `AutoModelForSeq2SeqLM` class to be able to generate new tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the model, we can take a look at the model's configuration. The configuration contains all the hyperparameters of the model. The configuration is available as a dictionary, and we can access the values by using the attribute notation.\n",
    "\n",
    "You can find the model's configuration object in the `config` attribute of the model. Or, if you only need the configuration, you can directly load it using the `AutoConfig` class.\n",
    "\n",
    "```python\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"google-t5/t5-base\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 3072,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.45.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of information going on here. We can just take a few key aspects:\n",
    "\n",
    "- `d_kv` = 64. This is the dimension of keys $d_k$ (so also queries) and values $d_v$ in the attention mechanism. It is common, to keep things simple, to use the same number of dimensions for keys, queries, and values (even though this is not strictly necessary). \n",
    "- `d_model` = 768. This is the dimension of the output of each transformer block.\n",
    "- `d_ff` = 3072. This is the dimension of the feedforward network in each transformer block. We will see that the feedforward network is composed of two linear layers with a ReLU activation in between (`d_model -> d_ff -> d_model`).\n",
    "- `num_layers` = 12. This is the number of transformer blocks in the model (both encoder and decoder).\n",
    "- `num_heads` = 12. This is the number of attention heads in the multi-head attention mechanism. Each head will produce a different representation of the input text, and the results will be concatenated together. Remember that the output of each attention head is concatenated. We have 12 heads, each producing a 64-dimensional output, so the final output will be 12 * 64 = 768-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the model to better understand its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has a `shared` Embedding layer, an `encoder` and a `decoder`, and a final `lm_head`.\n",
    "\n",
    "We can look into the token embedding first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can verify that this `shared` embedding layer, as the name suggests, is shared between the encoder and the decoder.\n",
    "\n",
    "We do so by checking the id of the embedding layer with the embedding layers found in the encoder and decoder (`model.encoder.embed_tokens` and `model.decoder.embed_tokens`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model.shared) == id(model.encoder.embed_tokens) and id(model.shared) == id(model.decoder.embed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we note that the embedding layer has 32128 tokens. \n",
    "\n",
    "This is not exactly the number of tokens we saw before (32100). The extra 28 tokens are \"leftovers\". 32128 = 251 * 128 is a more \"GPU friendly\" number, and it's used to speed up the computation (same reason why we often see batch sizes that are powers of 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"chair\",\n",
    "    \"table\",\n",
    "    \"plate\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"horse\",\n",
    "    \"goat\",\n",
    "    \"sheep\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3533,   953,  3829, 10821, 14987,  4952, 18174, 15184,  1712,  1782])\n",
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenizer(words, return_tensors=\"pt\", padding=True)[\"input_ids\"][:, 0]\n",
    "print(word_tokens)\n",
    "token_embeddings = model.shared(word_tokens).cpu().detach().numpy()\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(token_embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(X_pca[i, 0]+5, X_pca[i, 1]+5, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(token_embeddings)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "cax = ax.imshow(cosine_similarity(token_embeddings), cmap=\"coolwarm\")\n",
    "fig.colorbar(cax)\n",
    "plt.xticks(range(len(words)), words, rotation=90)\n",
    "plt.yticks(range(len(words)), words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's encoder contains various transformer layers (each one attention, feed-forward, and normalization layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 768)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `block` attribute is a list of `T5Block` modules, and contains the actual transformer layers. Indeed, we can check (as previously stated) that we have 12 encoder layers in the encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encoder.block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the modules inside of the `block` is composed of a `T5LayerSelfAttention` module (attention layer), followed by a `T5LayerFF` module (feed-forward layer).\n",
    "\n",
    "Remember that there are some other details (e.g., normalization layers, or dropouts) that are shown below, but we will not discuss them in detail here.\n",
    "\n",
    "As a reminder, this is the architecture of a single encoder block. \n",
    "\n",
    "![encoder.png](encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Block(\n",
       "  (layer): ModuleList(\n",
       "    (0): T5LayerSelfAttention(\n",
       "      (SelfAttention): T5Attention(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (relative_attention_bias): Embedding(32, 12)\n",
       "      )\n",
       "      (layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): T5LayerFF(\n",
       "      (DenseReluDense): T5DenseActDense(\n",
       "        (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.block[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the $W_q$, $W_k$, and $W_v$ matrices are 768x768. But, we stated that each attention is 64-dimensional, so they should be 768x64!\n",
    "\n",
    "However, remember that we have 12 heads: instead of producing 12 different heads, we instead efficiently represent all matrices inside of a single matrix. In addition, there is no need for concatenating the results: the output will already be the concatenation of all heads.\n",
    "\n",
    "The $W_o$ matrix is 768x768, i.e. `d_kv`*`num_heads` x `d_model`, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "We can inspect the decoder in the same way. Remember the decoder's architecture!\n",
    "\n",
    "![decoder.png](decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Block(\n",
       "  (layer): ModuleList(\n",
       "    (0): T5LayerSelfAttention(\n",
       "      (SelfAttention): T5Attention(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (relative_attention_bias): Embedding(32, 12)\n",
       "      )\n",
       "      (layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): T5LayerCrossAttention(\n",
       "      (EncDecAttention): T5Attention(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): T5LayerFF(\n",
       "      (DenseReluDense): T5DenseActDense(\n",
       "        (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.block[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we find the expected architecture. \n",
    "\n",
    "An initial self-attention module, followed by a multi-head attention one (`T5LayerCrossAttention`), and finally a feed-forward module.\n",
    "\n",
    "Note that, although there is a dedicated class for cross-attention, the module still makes use of the same `T5Attention` class we also used for self-attention. This is because nothing changes in the attention mechanism itself: only the inputs are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that the model's `lm_head` is a linear layer that takes the output of the decoder (768-dimensional) and maps it to the vocabulary size (32128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=32128, bias=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token generation\n",
    "\n",
    "Let's now focus on the token generation process.\n",
    "\n",
    "Remember, we are working with an encoder-decoder architecture. The encoder processes the input text, and the decoder generates the output text.\n",
    "\n",
    "The input for the encoder is the tokenized input text. We also need to specify an input for the decoder. The decoder's input will be the currently generated sequence thus far. For the first iteration, there is nothing already generated, so we need to specify a special token to indicate the beginning of the sequence (BOS). For T5, the token will be `<pad_token>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_sentence = \"translate from english to german: hello, how are you?\"\n",
    "\n",
    "tokens = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "decoder_input_ids = torch.tensor([[ tokenizer.pad_token_id ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder's input\n",
      "tensor([[13959,    45, 22269,    12, 13692,    10, 21820,     6,   149,    33,\n",
      "            25,    58,     1]])\n",
      "\n",
      "Decoder's input\n",
      "tensor([[0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder's input\")\n",
    "print(tokens[\"input_ids\"])\n",
    "\n",
    "print()\n",
    "print(\"Decoder's input\")\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the first output token (after `<pad_token>`), we call the model's __call__ method (or `forward` method) with the input text and the decoder's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])\n"
     ]
    }
   ],
   "source": [
    "# **tokens passes the dictionary as keyword arguments (input_ids=..., attention_mask=...)\n",
    "output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get three outputs:\n",
    "\n",
    "- `logits`: The logits for each token in the vocabulary. The logits are the output of the linear layer (the `lm_head`) that maps the decoder's output to the vocabulary size. The logits are used to compute the probabilities of each token.\n",
    "\n",
    "- `past_key_values`: The past key-values of the decoder. This is used to speed up the generation process for future tokens. Remember: the decoder is autoregressive, so we generate one token at a time. Since each token can only pay attention to past tokens, it means that the predictions made for earlier tokens will not change. Thus, we can cache them and not re-compute them. So, if we pas `past_key_values` to the model the next time we call it, it will be faster.\n",
    "\n",
    "- `encoder_last_hidden_state`: The hidden states of the last layer of the encoder. This is not used for generating the next token, but we may find it useful for other tasks (e.g., summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32128])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the logits is 1x1x32128. The first dimension is the batch size, the second dimension is the number of tokens in the output sequence (we are still at a single generated token), and the third dimension is the number of tokens in the vocabulary.\n",
    "\n",
    "So what's the next word going to be? We can use greedy decoding to take the token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max probability token: 2501\n",
      "Corresponding token: Hall\n"
     ]
    }
   ],
   "source": [
    "max_proba_token = output.logits[0,0].argmax()\n",
    "print(\"Max probability token:\", max_proba_token.item())\n",
    "print(\"Corresponding token:\", tokenizer.decode(max_proba_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're on the right track. We have a new token, which we can add to our `decoder_input_ids` tensor and continue with the next iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 2501]])\n"
     ]
    }
   ],
   "source": [
    "# .view() acts like .reshape(), but assumes that the tensor is contiguous in memory (& is faster)\n",
    "decoder_input_ids = torch.hstack([decoder_input_ids, max_proba_token.view(1, 1)])\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 32128])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the output has shape 1x2x32128. We have generated two tokens! \n",
    "\n",
    "The first one, remember, will necessarily be the same as the one we produced before, since the masking mechanism prevents the model from seeing any following token. \n",
    "\n",
    "But just to be sure, let's decode both tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids tensor([2501,   32,   32,   32,   32,   32,    6,    6,    6, 2501, 2501, 2501,\n",
      "        2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501,\n",
      "        2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501,\n",
      "        2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501, 2501,\n",
      "        2501, 2501])\n",
      "Mapped tokens ['▁Hall', 'o', 'o', 'o', 'o', 'o', ',', ',', ',', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall', '▁Hall']\n",
      "Decoded string Hallooooo,,, Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall Hall\n"
     ]
    }
   ],
   "source": [
    "# 0 -> first (and only) batch\n",
    "max_proba_tokens = output.logits[0].argmax(axis=1)\n",
    "print(\"Token ids\", max_proba_tokens)\n",
    "print(\"Mapped tokens\", list(map(reverse_vocab.get, max_proba_tokens.tolist())))\n",
    "print(\"Decoded string\", tokenizer.decode(max_proba_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are onto something. We could go ahead and generate more tokens manually. \n",
    "\n",
    "However, we can implement our own generation loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Hallo, wie erlebe ich sie?</s>']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy deoding\n",
    "tokenizer.batch_decode(model.generate(**tokens, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2501])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Hall\n",
      "Step 2: Hallo\n",
      "Step 3: Hallo,\n",
      "Step 4: Hallo, wie\n",
      "Step 5: Hallo, wie sind\n",
      "Step 6: Hallo, wie sind Sie\n",
      "Step 7: Hallo, wie sind Sie?\n",
      "Step 8: Hallo, wie sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder_input_ids = torch.tensor([[ tokenizer.pad_token_id ]])\n",
    "\n",
    "max_length = 50\n",
    "i = 0\n",
    "\n",
    "while i < max_length and decoder_input_ids[0,-1] != tokenizer.eos_token_id:\n",
    "    output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
    "    max_proba_tokens = output.logits[0].argmax(axis=1)\n",
    "    print(f\"Step {i+1}: {tokenizer.decode(max_proba_tokens)}\")\n",
    "    decoder_input_ids = torch.hstack([decoder_input_ids, max_proba_tokens[-1].view(1, 1)])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are introducing a `max_length` parameter. This is just in case, to prevent the model from generating an infinite sequence.\n",
    "\n",
    "Now, turns out that the model has a `generate` method that does exactly what we just did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Hallo, wie sind Sie?</s>']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tokens = model.generate(**tokens, max_length=max_length)\n",
    "tokenizer.batch_decode(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vectors =            model.decoder.embed_tokens.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.098358884, 18.803375)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.mean(), vectors.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515.54474"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((vectors**2).sum(axis=1))**0.5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10169"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2138"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()[\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3208"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()[\"pen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21410335918305048"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_dog = vectors[10169]\n",
    "v_cat = vectors[2138]\n",
    "v_pen = vectors[3208]\n",
    "\n",
    "(v_dog * v_cat).sum() / (((v_dog**2).sum() * (v_cat**2).sum())**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09220335227181164"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v_dog * v_pen ).sum() / (((v_dog**2).sum() * (v_pen**2).sum())**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-22.2360, -10.7458, -15.3099,  ..., -41.3863, -41.2533, -41.2233]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokens, decoder_input_ids=decoder_input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.final_layer_norm model(**tokens, decoder_input_ids=decoder_input_ids).last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
