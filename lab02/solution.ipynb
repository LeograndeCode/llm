{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 02 - Large Language Models**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lab 02:** Introduction to the Attention Mechanism and NLP Pipelines with Hugging Face\n",
    "\n",
    "In this lab, we will better understand the mechanics of the **Attention Mechanism**, a critical component in many modern deep learning models, particularly in the field of **Natural Language Processing (NLP)** and **Large Language Models (LLMs)**. This session will provide a conceptual and practical understanding of how attention works and its role in enhancing model performance.\n",
    "\n",
    "You will also be introduced to **Hugging Face**, one of the most widely-used libraries for NLP, and explore how to leverage pre-trained models for various language tasks. Throughout the lab, you will implement a basic **NLP pipeline**, applying attention-based models to process and analyze textual data.\n",
    "\n",
    "*The lab is structured to gradually build your familiarity with these tools. Initially, you will learn about the underlying principles of the attention mechanism and then move on to practical exercises that guide you through setting up your first NLP model using Hugging Face. By the end of this lab, you should be comfortable working with attention in PyTorch and building simple NLP pipelines.* To be changed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# set seed \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Exercise 1: Building a Simple Attention Mechanism for a Time Series Problem**\n",
    "\n",
    "In this exercise, the goal is to design and implement a **simple attention mechanism** and apply it to a time series prediction problem. You will work with a synthetic time series dataset that features flat sequences with periodic jumps, and explore how attention can help the model focus on the most relevant time steps for better prediction accuracy.\n",
    "The attention mechanism allows the model to assign varying importance to different parts of the input sequence, helping it to better capture temporal dependencies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a Synthetic Time Series Dataset with Jumps**\n",
    "\n",
    "In this section, we define a **custom dataset class** that generates synthetic time series data with jumps at regular intervals. This class, `TimeSeriesDataset`, is designed to simulate a flat time series with pseudorandom spikes introduced at specific points. \n",
    "Each time series begins as a flat line and has a jump introduced at a random point within the first few time steps. Additional jumps occur at regular intervals, alternating between small and large jumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Synthetic Time Series Dataset (Flat with jumps)\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_len, num_samples):\n",
    "        val = 3\n",
    "        initial_jump = 5 \n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            time_series = np.zeros(seq_len + 1)\n",
    "            \n",
    "            # Introduce the first jump at a random point between 0 and val\n",
    "            first_jump = np.random.randint(0, val)\n",
    "            time_series[first_jump] += np.random.rand() + initial_jump  # Add a jump with some noise\n",
    "            \n",
    "            # Continue to introduce jumps every val timesteps after the first jump\n",
    "            for i in range(first_jump + val, seq_len + 1, val):\n",
    "                jump = 0 if i % 2 == 1 else initial_jump # Alternate between 0 and initial_jump\n",
    "                time_series[i] += np.random.rand() + jump  # Add a jump with some noise\n",
    "            \n",
    "            self.data.append(time_series)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        series = self.data[idx]\n",
    "        return torch.tensor(series[:-1], dtype=torch.float32), torch.tensor(series[-1], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample of the dataset\n",
    "\n",
    "dataset = TimeSeriesDataset(seq_len=40, num_samples=1)\n",
    "x, y = dataset[0]\n",
    "plt.plot(range(40), x, label='x')\n",
    "plt.title(f\"Example Time Series Data Point\\n y: {y}\")\n",
    "plt.scatter(40, y, color='r', label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing the Scaled Dot-Product Attention Mechanism and Seq2Seq Model**\n",
    "\n",
    "This section defines the **Scaled Dot-Product Attention Mechanism**, a core component in modern neural network models like Transformers. The attention mechanism operates by computing a weighted sum of the values, where the weights are determined by the similarity between the query and key. The similarity is scaled by the dimensionality of the key (`d_k`) to stabilize gradients.\n",
    "\n",
    "The attention mechanism involves the following steps:\n",
    "1. **Compute Scores**: The similarity between the query and key is calculated by taking their dot product.\n",
    "2. **Scale the Scores**: The scores are divided by the square root of `d_k` to prevent large values from overwhelming the softmax.\n",
    "3. **Masking** (Optional): A mask can be applied to prevent certain positions from influencing the attention weights. We will better understand the usefulness of masking in the second exercise, for now we do not care about it.\n",
    "4. **Softmax and Output**: The attention weights are calculated using softmax, and the final output is computed by taking the weighted sum of the values.\n",
    "\n",
    "Following this, a **Seq2Seq Model** with the scaled dot-product attention mechanism is defined. This model uses a Long-Short Term Memory (LSTM) to process input sequences and generate hidden states. The hidden state of the last LSTM cell serves as the query for the attention mechanism, while the entire sequence of hidden states is used as the keys and values.\n",
    "\n",
    "The attention mechanism helps the model focus on the most relevant parts of the input sequence when making predictions. After applying the attention, the context vector is fed into a fully connected layer to produce the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaled Dot-Product Attention Mechanism**\n",
    "\n",
    "In this section, we define the **Scaled Dot-Product Attention** mechanism, a key component of the Transformer architecture that allows the model to focus on specific parts of the input sequence when making predictions.\n",
    "\n",
    "The attention mechanism operates in the following steps:\n",
    "\n",
    "- **Score Calculation**: The attention scores are calculated by taking the dot product between the query and the transposed key matrices. This operation measures how relevant each position in the key is to the query. The scores are scaled by the square root of the dimensionality of the key (`d_k`) to avoid large gradients, which could destabilize the training process.\n",
    "\n",
    "- **Masking (Optional)**: If a mask is provided, it is applied to the scores by setting specific elements to a very large negative value. This prevents the model from attending to certain positions, which is particularly useful when handling padded sequences in time series or natural language tasks. For this particular problem the mask is not required, so it is not present in the implementation.\n",
    "\n",
    "- **Softmax**: After computing the scaled scores, the softmax function is applied to normalize the scores into probabilities. This step ensures that the attention weights sum to 1 across each sequence position, allowing the model to attend to specific parts of the input more heavily.\n",
    "\n",
    "- **Attention Output**: Finally, the attention weights are used to compute a weighted sum of the value matrix, generating the output. This allows the model to focus on important information in the sequence and ignore less relevant parts.\n",
    "\n",
    "The attention mechanism returns both the final output and the attention weights, providing insight into which parts of the input the model is focusing on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Scaled Dot-Product Attention Mechanism\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, seq_len, d_k, other_positional_encodings_present):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.seq_len = seq_len\n",
    "        self.other_positional_encodings_present = other_positional_encodings_present\n",
    "\n",
    "        self.rel_layer = torch.randn(1, self.seq_len, self.seq_len, requires_grad=True) # Create a learnable relative positional encoding\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Compute the attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) # Scaled Dot-Product\n",
    "        \n",
    "        # Add relative positional encodings if present\n",
    "        if not self.other_positional_encodings_present:\n",
    "            scores = scores + self.rel_layer\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attention Mechanism in Transformer**\n",
    "\n",
    "This section defines the **Attention** mechanism, a core component of the Transformer model, which allows the model to focus on different parts of the input sequence simultaneously. This mechanism helps the model capture multiple relationships and patterns within the data.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "- **Linear Layers for Query, Key, and Value**: The input sequence is first transformed into query (`Q`), key (`K`), and value (`V`) vectors using linear transformations. These vectors represent the input in different spaces, allowing the model to calculate attention scores.\n",
    "\n",
    "- **Attention Head**: The `AttentionHead` class implements a single attention head in a Transformer model. It transforms the input into query, key, and value vectors, computes attention scores, applies a softmax function to obtain attention weights, and produces the final attention output by computing a weighted sum of the value vectors.\n",
    "\n",
    "- **Scaled Dot-Product Attention**: The head performs scaled dot-product attention by calculating the attention scores between the query and key vectors, scaling them, and applying a softmax function. The resulting attention weights are used to compute a weighted sum of the value vectors, producing the attention output.\n",
    "\n",
    "- **Output Linear Layer**: The output from the attention head is passed through a final linear layer to produce the final output.\n",
    "\n",
    "The `AttentionHead` class returns both the final output and the attention weights, enabling a deeper understanding of the model’s focus at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Multi-Head Attention mechanism (used in Transformer)\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, other_positional_encodings_present):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.d_k = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Define linear layers for query, key, and value\n",
    "        self.query_layer = nn.Linear(d_model, d_model)\n",
    "        self.key_layer = nn.Linear(d_model, d_model)\n",
    "        self.value_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        self.scaled_dot_attention = ScaledDotProductAttention(self.seq_len, self.d_k, other_positional_encodings_present)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "                \n",
    "        # Apply linear layers to query, key, value\n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(key)\n",
    "        V = self.value_layer(value)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_attention(Q, K, V)\n",
    "\n",
    "        attention_output = attention_output.squeeze(1)       \n",
    "        output = self.fc_out(attention_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Encoder Block**\n",
    "\n",
    "The **TransformerEncoderBlock** defines a single block in the Transformer architecture, combining the attention mechanism with a feed-forward neural network and normalization layers. This block processes input sequences to capture complex relationships while maintaining stability during training through normalization and dropout.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "- **Multi-Head Attention**: The input sequence is first passed through the **MultiHeadAttention** mechanism. This allows the model to focus on different parts of the sequence in parallel, attending to multiple aspects of the input. The multi-head attention output is then combined with the original input (via residual connection) after applying dropout for regularization.\n",
    "\n",
    "- **Layer Normalization (Norm1)**: After multi-head attention, the output is passed through the first layer normalization step (`norm1`). This ensures that the model’s representations remain stable by normalizing the output along the feature dimension.\n",
    "\n",
    "- **Feed-Forward Network (FFN)**: The output of the attention layer is further processed through a fully connected feed-forward neural network. This network consists of two linear layers with a ReLU activation function in between, allowing the model to capture nonlinear relationships. The `dim_feedforward` parameter controls the dimensionality of the hidden layer in the feed-forward network.\n",
    "\n",
    "- **Layer Normalization (Norm2)**: After the feed-forward network, another residual connection is applied, followed by a second layer normalization (`norm2`). This helps stabilize the output of the block and prevent vanishing or exploding gradients during training.\n",
    "\n",
    "- **Dropout**: Dropout is applied both after the attention mechanism and the feed-forward network to prevent overfitting by randomly setting some activations to zero during training.\n",
    "\n",
    "This encoder block is a key building block in the Transformer model, enabling it to capture both local and global dependencies in the input sequence through attention and feed-forward networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic Transformer Encoder block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, seq_len, other_positional_encodings_present, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = AttentionHead(d_model, seq_len, other_positional_encodings_present)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention head\n",
    "        attention_output, attention_weights = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        output = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Model for Time Series Prediction**\n",
    "\n",
    "This section defines a **Transformer model** tailored for time series prediction. The model processes sequential data by combining embedding, positional encoding, and stacked Transformer encoder layers. It predicts a single output value based on the time series input.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "- **Embedding Layer**: The time series input, which has a shape of `[batch_size, seq_len, input_size]`, is first passed through a linear embedding layer. This maps the input features to a higher-dimensional space with size `d_model`. The embedding allows the model to work in a more expressive feature space.\n",
    "\n",
    "- **Positional Encoding**: Since Transformers are designed to process sequences without a built-in sense of order, positional encoding is added to provide information about the position of each time step in the sequence. This ensures the model can understand the temporal order of the input sequence. The positional encoding matrix is added to the embedded input data.\n",
    "\n",
    "- **Transformer Encoder Layers**: The core of the model consists of multiple Transformer encoder blocks stacked together. Each encoder block contains a multi-head attention mechanism and a feed-forward neural network. The input data is passed through these layers sequentially. The model maintains attention weights from each layer, allowing insights into how the model attends to different parts of the sequence.\n",
    "\n",
    "- **Output Layer**: After processing the sequence through the encoder layers, the final output is computed by applying a linear layer to the last time step of the sequence. The model focuses on the last time step to predict a single output value, suitable for tasks like time series regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, dim_feedforward, num_layers, seq_len, positional_encoding = None, dropout=0.1, max_len=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "\n",
    "        # Embedding for the time series input\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding for time series data\n",
    "        if self.positional_encoding is not None:\n",
    "            self.pos_encoder = self.positional_encoding(d_model, max_len)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(d_model, dim_feedforward, seq_len, False if positional_encoding is None else True, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # Output layer to predict a single value\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # Embed the input time series data\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_len, d_model]    \n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x) if self.positional_encoding is not None else x\n",
    "        \n",
    "        # List to store attention weights from all layers\n",
    "        all_attention_weights = []\n",
    "\n",
    "        # Pass through each Transformer encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            x, attention_weights = layer(x)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "\n",
    "        # Apply final linear layer to get the output\n",
    "        output = self.fc_out(x[:, -1])  # Take the output of the last time step (sequence regression)\n",
    "        return output, all_attention_weights  # Return the final output and the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1  # Number of input features\n",
    "seq_len = 40 # Length of the time series sequence\n",
    "\n",
    "d_model = 64  # Embedding dimension\n",
    "dim_feedforward = 257  # Feedforward network dimension\n",
    "num_layers = 3  # Number of Transformer encoder layers\n",
    "\n",
    "model = Transformer(input_size, d_model, dim_feedforward, num_layers, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, target in dataloader:\n",
    "            inputs = inputs.unsqueeze(2)  # Add feature dimension (batch_size, seq_len, input_size)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
    "    \n",
    "    # Plot the loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss During Training')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Number of samples in each batch\n",
    "num_samples = 1000 # Number of samples in the dataset\n",
    "num_epochs = 15 # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataLoader\n",
    "dataset = TimeSeriesDataset(seq_len, num_samples)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualization of Normalized Attention Weights**\n",
    "\n",
    "This plot displays the **Input Series** (blue line) along with the corresponding **Normalized Attention Weights** (orange dashed line). The normalized attention weights highlight the time steps that the Transformer model focuses on during the prediction process.\n",
    "\n",
    "- The **normalized attention weights** are concentrated around time step 35, indicating that the model considers this region of the time series to be particularly important when making its prediction.\n",
    "- The true target value is `5.51`, while the model's predicted value is `2.80`, showing that the model has learned to attend to certain key points in the sequence but still underestimates the final target.\n",
    "\n",
    "This visualization provides insight into how the model allocates attention across different parts of the time series and illustrates its prediction process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Visualize the Attention Weights\n",
    "def visualize_attention(model, dataset):\n",
    "    model.eval()\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, target = dataset[i]\n",
    "        if target != 0: \n",
    "            # Reshape the input for the Transformer: [batch_size, seq_len, input_size]\n",
    "            inputs = inputs.unsqueeze(0).unsqueeze(2)\n",
    "            outputs, attention_weights = model(inputs)\n",
    "            break\n",
    "\n",
    "    # Plot the input time series (after undoing the batch and input dimension)\n",
    "    plt.plot(inputs.permute(1, 0, 2).squeeze().numpy(), label=\"Input Series\")\n",
    "\n",
    "    # The attention weights are returned from all layers, so let's use the attention from the last layer\n",
    "    attention_weights = attention_weights[-1][0].detach().numpy()  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "    # Normalize attention weights for better visibility\n",
    "    normalized_attention_weights = (attention_weights - np.min(attention_weights)) / (np.max(attention_weights) - np.min(attention_weights))\n",
    "    \n",
    "    \n",
    "    # Plot the normalized attention weights\n",
    "    plt.plot(normalized_attention_weights.mean(axis=0), label=\"Normalized Attention Weights\", linestyle='--')\n",
    "    plt.scatter(seq_len, target, color='r', label='True Target')\n",
    "    plt.scatter(seq_len, outputs[-1].item(), color='g', label='Predicted Target')\n",
    "    plt.title(f\"Example of a prediction:\\nTrue Target: {target.item():.2f}, Predicted: {outputs[-1].item():.2f}\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_attention(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Positional Encoding for Time Series Data**\n",
    "\n",
    "In this section, we define a **PositionalEncoding** class that adds positional information to the input data. Positional encodings are essential for the Transformer model to capture the order of the sequence since the attention mechanism itself is permutation-invariant.\n",
    "\n",
    "The **PositionalEncoding** class creates a matrix of shape `(max_len, d_model)` where each position in the sequence is represented by a unique combination of sine and cosine functions. These functions vary with different frequencies based on the position in the sequence, which helps the model distinguish between different time steps.\n",
    "\n",
    "- **Sine and Cosine Encodings**: The positional encodings alternate between sine and cosine functions for even and odd indices in the embedding dimension, allowing the model to encode sequential information effectively.\n",
    "- **Batch Compatibility**: The positional encoding matrix is reshaped and registered as a buffer, meaning it is not updated during backpropagation but is available during forward passes.\n",
    "- **Applying the Positional Encoding**: During the forward pass, the positional encoding is added to the input sequence to give each time step a unique representation based on its position.\n",
    "\n",
    "By using this approach, the Transformer model can learn both the content and the position of each element in the input sequence, which is crucial for time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute sine and cosine positional encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices in the encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices in the encoding\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Reshape for batch compatibility\n",
    "        self.register_buffer('pe', pe)  # Register pe as a buffer so it doesn't get updated during backpropagation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input\n",
    "        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(input_size, d_model, dim_feedforward, num_layers, seq_len, PositionalEncoding)\n",
    "\n",
    "# Instantiate the criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Visualize the Attention Weights\n",
    "def visualize_attention(model, dataset):\n",
    "    model.eval()\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, target = dataset[i]\n",
    "        if target != 0: \n",
    "            # Reshape the input for the Transformer: [batch_size, seq_len, input_size]\n",
    "            inputs = inputs.unsqueeze(0).unsqueeze(2)\n",
    "            outputs, attention_weights = model(inputs)\n",
    "            break\n",
    "\n",
    "    # Plot the input time series (after undoing the batch and input dimension)\n",
    "    plt.plot(inputs.permute(1, 0, 2).squeeze().numpy(), label=\"Input Series\")\n",
    "\n",
    "    # The attention weights are returned from all layers, so let's use the attention from the last layer\n",
    "    attention_weights = attention_weights[-1][0].detach().numpy()  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "    # Normalize attention weights for better visibility\n",
    "    normalized_attention_weights = (attention_weights - np.min(attention_weights)) / (np.max(attention_weights) - np.min(attention_weights))\n",
    "    \n",
    "    \n",
    "    # Plot the normalized attention weights\n",
    "    plt.plot(normalized_attention_weights.mean(axis=0), label=\"Normalized Attention Weights\", linestyle='--')\n",
    "    plt.scatter(seq_len, target, color='r', label='True Target')\n",
    "    plt.scatter(seq_len, outputs[-1].item(), color='g', label='Predicted Target')\n",
    "    plt.title(f\"Example of a prediction:\\nTrue Target: {target.item():.2f}, Predicted: {outputs[-1].item():.2f}\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_attention(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 2: Loading a Pre-Trained Model and Tokenizer with HuggingFace**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving a Pre-trained Model from Hugging Face\n",
    "\n",
    "Hugging Face’s `transformers` library provides a wide range of pre-trained models for Natural Language Processing (NLP) tasks. These models, trained on massive datasets, can be easily loaded and fine-tuned for specific tasks such as text classification, translation, summarization, and more. Among these models is **BERT** (Bidirectional Encoder Representations from Transformers), one of the most popular encoder-only transformer models.\n",
    "\n",
    "One of the core functionalities of Hugging Face is the ability to retrieve models directly from their model hub, where thousands of **pre-trained** models are available. Each model is identified by a model name or a repository path, and it comes with a pre-trained tokenizer. The tokenizer is responsible for converting raw text into numerical representations (tokens) that the model can process, ensuring the text can be understood by the model.\n",
    "\n",
    "The Hugging Face model hub provides models for various tasks, including:\n",
    "- **Text Classification**: Sentiment analysis, topic classification, etc.\n",
    "- **Question Answering**: Answering questions based on a provided context.\n",
    "- **Summarization**: Generating concise summaries of input text.\n",
    "- **Translation**: Translating text from one language to another.\n",
    "- **Text Generation**: Generating new text based on an input prompt.\n",
    "\n",
    "In this exercise, we will be using the **`google-bert/bert-base-uncased`** checkpoint, a pre-trained BERT model developed by Google. This model is uncased, meaning it treats uppercase and lowercase letters the same, which helps it generalize better for tasks where case sensitivity is not crucial. BERT is trained using both left and right context, making it powerful for a variety of NLP tasks such as classification, named entity recognition, and question answering. \n",
    "\n",
    "\n",
    "#### Steps to Retrieve a Pre-trained Model:\n",
    "1. **Load the Pre-trained Model and Tokenizer**: The `AutoModel` and `AutoTokenizer` classes allow for easy loading of any pre-trained model and its corresponding tokenizer from the Hugging Face model hub. Models are specified by their model name or repository path. For this exercise, we’ll load `google-bert/bert-base-uncased`, a pre-trained BERT model designed for a wide range of natural language processing tasks.\n",
    "   \n",
    "2. **Tokenize Input Text**: Before feeding text into the model, it needs to be tokenized using the pre-trained **BERT tokenizer**. This step converts raw text into token IDs that the model can interpret. The BERT tokenizer uses **WordPiece tokenization** to handle out-of-vocabulary words and efficiently represent subwords. For example, \"unhappiness\" would be split into subword tokens like `[\"un\", \"##happy\", \"##ness\"]`.\n",
    "\n",
    "3. **Perform Inference**: Once the input is tokenized, it can be passed to the BERT model to perform inference. Depending on the task (e.g., classification, question answering), the model outputs logits or hidden states. For classification tasks, the raw logits can be transformed into probabilities using a softmax function, and the label with the highest probability represents the model's prediction.\n",
    "\n",
    "Below is an example showing how to retrieve and use a pre-trained BERT model from Hugging Face for various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet to retrieve a model from Hugging Face\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Define the model name or path from Hugging Face\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we define a sample text that we will use for sentiment analysis **TODO**\n",
    "\n",
    "The tokenizer is then used to process the input text. The `tokenizer` converts the raw sentence into a format that the model can understand by breaking it down into tokens and converting them into numerical IDs. In this case, the method `tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)` ensures that the output is in the correct tensor format (`return_tensors=\"pt\"` for PyTorch), applies padding to ensure consistent input length across batches, and truncates the text if it exceeds the model's maximum sequence length. The output includes both **input IDs**, which are the tokenized numerical representations of the words, and an **attention mask**, which indicates which tokens should be attended to (where `1` signifies real tokens and `0` marks padding tokens). \n",
    "\n",
    "This ensures that only the relevant tokens are processed by the model, with the attention mask ignoring any padding that may have been added. By printing both the token IDs and the attention mask, we can inspect how the text has been prepared for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text for sentiment analysis\n",
    "text = \"The dog ate the food because it was hungry\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", inputs[\"input_ids\"])\n",
    "print(\"Attention Mask:\", inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Print the token IDs\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we pass the tokenized input (which includes the attention mask) to the model for inference. The input is fed into the model using `model(**inputs)`, which performs forward propagation to generate the output. The model’s output contains multiple components, but in this case, we are specifically interested in the **logits**, which are the raw, unnormalized predictions made by the BERT model.\n",
    "\n",
    "The **logits** represent the model's confidence in each class (for example, sentiment categories such as Negative, Neutral, and Positive) before applying any normalization. However, since the model hasn't been fine-tuned for a specific task yet, these logits won’t provide meaningful or reliable predictions at this point. \n",
    "\n",
    "For now, the output doesn't hold significant meaning, but this will be explored in detail in the next lab when we fine-tune the model for specific tasks. **For today's lab, we will limit ourselves to inference only in order to assess the attention weights of the model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the tokenized input (including attention mask) to the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the logits (raw predictions) from the model output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Apply softmax to get the predicted probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# Print the predicted probabilities for each sentiment class\n",
    "print(\"Softmax probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we focus on extracting and examining the **attention weights** produced by the model during inference. Attention weights provide insights into how the model distributes its focus across different tokens in the input sequence. By examining these weights, we can understand which words the model considers important when making predictions, especially in tasks like coreference resolution or sentiment analysis.\n",
    "\n",
    "First, we extract the attention weights from the model’s output by accessing the `outputs.attentions` attribute. These attention weights are generated by the self-attention mechanism in transformer models like RoBERTa. Self-attention allows each token in the sequence to \"attend\" to other tokens, meaning it learns how much focus should be placed on surrounding words. This is crucial for capturing contextual relationships between words in a sentence.\n",
    "\n",
    "Next, we print out the **number of attention layers** in the model using `len(attentions)`. Transformer models typically have multiple layers, each containing its own set of attention heads. For instance, BERT and RoBERTa base models generally have 12 layers, each of which processes the input tokens with attention mechanisms to refine the model's understanding of the sentence structure.\n",
    "\n",
    "We also print the **shape of the attention weights** from the first layer with `attentions[0].shape`. This shape reveals key information about how the attention is structured:\n",
    "- The first dimension represents the **batch size** (usually 1 in this case, as we are processing one sentence).\n",
    "- The second dimension corresponds to the **number of attention heads** in that layer, which are independent mechanisms that attend to different parts of the input.\n",
    "- The third and fourth dimensions both represent the **sequence length**, meaning how many tokens are in the input. Each token in the sequence attends to every other token, resulting in an attention matrix where every token has a score representing its attention to all other tokens.\n",
    "\n",
    "By printing the shape of the attention weights, we get a clear understanding of the structure of the attention mechanism across different layers, heads, and tokens. This sets the foundation for visualizing or further analyzing how the model attends to specific words or entities in the input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the attention weights from the output\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Print the attention weights shape\n",
    "print(f\"Number of attention layers: {len(attentions)}\")\n",
    "print(f\"Shape of attention weights in the first layer: {attentions[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use **Matplotlib** and **Seaborn** to visualize the attention weights extracted from the first attention head of the first layer in the transformer model. Visualizing attention weights allows us to better understand how the model distributes focus across the tokens in a sequence, showing which tokens \"attend\" to each other.\n",
    "\n",
    "We create a **heatmap** to visualize the attention weights using **Seaborn’s** `heatmap()` function. A heatmap is an intuitive way to display how much attention each token pays to every other token in the sequence. The heatmap shows the attention matrix, where each row represents a token and each column represents how much attention that token places on other tokens. Darker shades indicate higher attention scores.\n",
    "\n",
    "By visualizing the attention matrix, we can observe the relationships between tokens in the input sentence, such as whether certain words attend heavily to specific other words, providing insights into how the model understands the sentence contextually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert attention weights to numpy array (taking the first attention head from the first layer)\n",
    "attention_layer_1 = attentions[0][0, 0].detach().numpy()\n",
    "\n",
    "# Plot the attention weights for the first layer, first attention head\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(attention_layer_1, annot=False, cmap=\"Blues\", xticklabels=tokens, yticklabels=tokens)\n",
    "plt.xlabel(\"Attention to Token\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.title(\"Attention Weights - Layer 1, Head 1\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
