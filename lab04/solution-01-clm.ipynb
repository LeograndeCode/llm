{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual Language Modeling (CLM) - Preprocess, Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore **Causal Language Modeling (CLM)**, which is a core task in training autoregressive language models like GPT-2. CLM is the process of predicting the next word in a sequence, given the previous words. This type of modeling forms the backbone of text generation tasks, where the model learns to generate coherent text by focusing only on previous tokens in the sequence.\n",
    "\n",
    "The lab is divided into three major sections:\n",
    "1. **Preprocessing**: Preparing a dataset and the labels for the CLM task and tokenizing them.\n",
    "2. **Training**: Fine-tuning a pre-trained language model like GPT-2 on a specific dataset using the CLM task.\n",
    "3. **Inference**: Evaluating the model’s performance by generating text based on input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csavelli/miniconda3/envs/hf_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Domain-Specific Dataset**:\n",
    "   - The first step in fine-tuning GPT-2 is to load a dataset that is specific to the domain of interest. In this case, we are using a publicly available **medical dataset** from the **PubMed** collection. PubMed contains a vast number of medical articles, and fine-tuning on such a dataset can help GPT-2 generate more accurate and context-specific medical text.\n",
    "   - The dataset we're using is `\"japhba/pubmed_simple\"`, which is a simplified version of PubMed data. This dataset can be easily accessed using the `datasets` library from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|██████████| 1.45k/1.45k [00:00<00:00, 12.4MB/s]\n",
      "Downloading data: 100%|██████████| 274M/274M [00:24<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 271M/271M [00:24<00:00, 11.3MB/s] \n",
      "Downloading data: 100%|██████████| 256M/256M [00:22<00:00, 11.3MB/s] \n",
      "Downloading data: 100%|██████████| 239M/239M [00:21<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 236M/236M [00:20<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 233M/233M [00:20<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 238M/238M [00:21<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 242M/242M [00:21<00:00, 11.2MB/s] \n",
      "Generating train split: 100%|██████████| 3116832/3116832 [00:08<00:00, 357352.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"japhba/pubmed_simple\", split=\"train\")\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Tokenize the Dataset**:\n",
    "   - The next step is to **tokenize** the data so that it can be processed by GPT-2.\n",
    "   - Since GPT-2 does not natively use a padding token, since it does not require fixed length inputs. For this reason, we will substite it with the EOS token as suggested by transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples['abstract'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Add labels to the Dataset for Next Token Prediction**:\n",
    "   - In this step, we will add the **labels** that will be used during the next token prediction task. \n",
    "   - In autoregressive language modeling, the **labels** represent the same sequence as the input, shifted one token to the right. This is because the model is trained to predict the next token in the sequence given the previous tokens.\n",
    "   - The shifting of the tokens is already handled automatically by the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the dataset\n",
    "def add_labels(samples):\n",
    "    samples[\"labels\"] = samples[\"input_ids\"].copy()\n",
    "    return samples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fine-Tune the GPT-2 Model**:\n",
    "   - Set up the model and finetune it using the medical dataset. \n",
    "   - The pipeline to be followed is the same that we have already seen in the previous lab (`lab03 - 01-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csavelli/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 02:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize GPT-2 Model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-medical-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Compare Text Generation Before and After Fine-Tuning**:\n",
    "   - Generate text using both the original pre-trained GPT-2 model and the fine-tuned model.\n",
    "   - Provide the same input prompt and observe the differences in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained and fine-tuned GPT-2 models\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-medical-finetuned\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = \"The patient presents with chest pain and shortness of breath.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Before Fine-Tuning (Pre-Trained GPT-2)**:\n",
      "The patient presents with chest pain and shortness of breath. The patient's left arm is twisted and the patient is bleeding. The patient is bleeding from the lower back. The patient is bleeding from the upper back. The patient is bleeding from the left side of his body. The patient is bleeding from his left hip. A chest pain is evident in the right chest. There is a pulse that is the same as the left pulse. The patient is bleeding from the left side of the chest. The\n"
     ]
    }
   ],
   "source": [
    "# Generate Text Before Fine-Tuning\n",
    "output_pretrained = pretrained_model.generate(input_ids, do_sample=True, max_length=100, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the Outputs for ComparisonS\n",
    "print(\"**Before Fine-Tuning (Pre-Trained GPT-2)**:\")\n",
    "print(generated_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**After Fine-Tuning (Fine-Tuned on Medical Dataset)**:\n",
      "The patient presents with chest pain and shortness of breath. He presents with a paresthesias and nausea. He has diarrhea and is irritable bowel syndrome (IBS). He is mildly irritable bowel syndrome (LBS).\n"
     ]
    }
   ],
   "source": [
    "# Generate Text After Fine-Tuning\n",
    "output_finetuned = finetuned_model.generate(input_ids, do_sample=True, max_length=100, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n**After Fine-Tuning (Fine-Tuned on Medical Dataset)**:\")\n",
    "print(generated_finetuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
