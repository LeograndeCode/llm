{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only architecture - BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk through the basics of the BERT architecture, how its tokenizer works, and how positional encoding is applied. \n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for NLP tasks. It has multiple layers of encoders, each containing:\n",
    "- Self-attention mechanism\n",
    "- Feed-forward neural networks\n",
    "- Layer normalization and residual connections\n",
    "\n",
    "We'll start by visualizing the structure of BERT's encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Embeddings Layer**\n",
    "The embeddings layer is responsible for creating representations of the input tokens. It includes:\n",
    "\n",
    "- **Word Embeddings**: Maps each of the 30,522 vocabulary tokens to a 768-dimensional vector.\n",
    "- **Position Embeddings**: Adds positional information to each token using a 512-length sequence of 768-dimensional vectors.\n",
    "- **Token Type Embeddings**: Adds embeddings to distinguish between segments (e.g., Sentence A and Sentence B) with 2 different token types.\n",
    "- **Layer Normalization and Dropout**: Normalizes the embeddings and applies dropout to prevent overfitting.\n",
    "\n",
    "### 2. **BERT Encoder**\n",
    "BERT's encoder is composed of 12 identical layers (for BERT-base, 24 for BERT-large), each called a **BERT Layer**. Each BERT layer consists of the following components:\n",
    "\n",
    "- **Self-Attention**:\n",
    "  - The self-attention mechanism is divided into multiple heads, each computing its own attention scores using three linear projections: Query (`Q`), Key (`K`), and Value (`V`). \n",
    "\n",
    "- **Self-Attention Output**:\n",
    "  - The output of the self-attention mechanism is passed through a dense (linear) layer that projects the output back to the original dimension of 768.\n",
    "\n",
    "- **Feed-Forward Neural Network**:\n",
    "  - Each BERT layer includes a position-wise feed-forward network that processes each token independently after the self-attention mechanism.\n",
    "  - The feed-forward network consists of two dense layers:\n",
    "    - The **intermediate layer** expands the dimensionality from 768 to 3072 using a linear transformation and applies a **GELU** (Gaussian Error Linear Unit) activation function for non-linearity.\n",
    "    - The **output layer** projects the 3072-dimensional vector back down to 768 dimensions, matching the input size.\n",
    "\n",
    "- **Residual Connections**:\n",
    "  - BERT uses residual (skip) connections around the self-attention and feed-forward components. The input to each sub-layer is added to its output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT Tokenization with WordPiece**\n",
    "\n",
    "BERT uses a **WordPiece tokenizer**, a subword tokenization technique. This tokenizer splits words into smaller subwords or tokens, allowing BERT to represent rare words as combinations of common subwords and prefixes.\n",
    "\n",
    "#### **How WordPiece Works**\n",
    "\n",
    "The WordPiece tokenizer splits a word based on its frequency in the training data:\n",
    "- **Common Words**: If a word is frequently used in the training corpus, it will likely be stored as a single token. For example, words like \"hello\" or \"sentence\" are common enough to exist in BERT's vocabulary as single tokens.\n",
    "- **Rare or Unknown Words**: For less common or out-of-vocabulary words, the tokenizer will break them into subword units or prefixes. These subwords are often combined with a special prefix `##` to indicate they are part of a larger word. \n",
    "\n",
    "Let’s see how the WordPiece tokenizer processes a sample sentence using BERT’s tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"hello, this is a sentence!\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This sentence is too cumbersome!\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens in BERT\n",
    "\n",
    "BERT uses several special tokens to structure the input and help the model understand the context and relationships between sequences. These tokens play an important role in various NLP tasks such as sentence classification, next sentence prediction, and more.\n",
    "\n",
    "#### 1. **[CLS] Token** (Classification Token)\n",
    "- The `[CLS]` token is added at the beginning of every input sequence. \n",
    "- BERT uses this token as a representation of the entire sequence. The hidden state of the `[CLS]` token after passing through BERT can be used for classification tasks (we are going to use it later!).\n",
    "\n",
    "#### 2. **[PAD] Token** (Padding Token)\n",
    "- The `[PAD]` token is used to pad sequences so they all have the same length within a batch. The attention mechanism then ignores these `[PAD]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"This is a short sentence.\"\n",
    "tokens_single = tokenizer(sentence1, return_tensors='pt', padding='max_length', max_length=10)\n",
    "\n",
    "# Display tokenized output and input IDs\n",
    "print(\"Single Sentence Tokenization:\")\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens_single['input_ids'][0]))\n",
    "print(\"Input IDs:\", tokens_single['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **[SEP] Token** (Separator Token)\n",
    "- The `[SEP]` token is used to separate segments (sentences) in BERT's input.\n",
    "- It is especially important for tasks like Next Sentence Prediction (NSP) where BERT needs to understand the relationship between two sentences.\n",
    "- It is also added at the end of the sequence to indicate its termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"But now it is a little longer.\"\n",
    "tokens_pair = tokenizer(sentence1, sentence2, return_tensors='pt', padding='max_length', max_length=20)\n",
    "\n",
    "# Display tokenized output and input IDs\n",
    "print(\"\\nTwo Sentences Tokenization:\")\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens_pair['input_ids'][0]))\n",
    "print(\"Input IDs:\", tokens_pair['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **[MASK] Token**\n",
    "- The `[MASK]` token is used during training for the Masked Language Modeling (MLM) task, where a percentage of tokens are masked and the model must predict them based on the context.\n",
    "- This token is rarely used during inference but is crucial in pre-training BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a mask token \n",
    "mask_token = tokenizer.mask_token \n",
    "mask_token_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "# Encode a text\n",
    "text = \"I do not know how the sentence will end.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "mask_position = 9\n",
    "encoded_input['input_ids'][0, mask_position] = mask_token_id\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encodings in Transformers\n",
    "\n",
    "There are two common approaches for implementing positional encodings:\n",
    "\n",
    "#### 1. **Static Sinusoidal Positional Encodings**\n",
    "\n",
    "- The one that we already saw in the previous laboratory. \n",
    "- **Advantages**:\n",
    "  - This method requires no additional parameters or learning during training.\n",
    "  - The same encoding can be applied across sequences of any length without adjustment.\n",
    "- **Disadvantages**:\n",
    "  - Since these encodings are static, they might not be as flexible or as specific as learned positional embeddings, which can adapt better to the data.\n",
    "\n",
    "#### 2. **Learned Positional Embeddings**\n",
    "\n",
    "- Instead of using a fixed, pre-defined encoding, learned positional embeddings are trained as part of the model, similar to how word embeddings are learned.\n",
    "- The model learns a set of positional vectors, where each vector corresponds to a position in the sequence (e.g., position 0, position 1, etc.). \n",
    "- **Advantages**:\n",
    "  - The model can change positional encodings to the data it is trained on, potentially capturing positional relationships more effectively.\n",
    "- **Disadvantages**:\n",
    "  - Learned positional embeddings introduce additional parameters, which increases the model size and training complexity.\n",
    "  - The learned embeddings are limited to a maximum sequence length (e.g., 512 for BERT).\n",
    "\n",
    "BERT uses the second approach: **learned positional embeddings**. During the training process, BERT learns a set of positional vectors corresponding to token positions up to a maximum length (typically 512). These embeddings are added to the token embeddings to provide position information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Position embeddings are stored in the embedding layer as 'position_embeddings.weight'\n",
    "positional_encodings = model.embeddings.position_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Compute cosine similarity between positional encodings\n",
    "cosine_sim = cosine_similarity(positional_encodings)\n",
    "\n",
    "# Plot the cosine similarity heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cosine_sim, cmap='viridis', cbar=True, vmin=0)\n",
    "plt.title('Cosine Similarity between BERT Positional Encoding Vectors')\n",
    "plt.xlabel('Vector 1')\n",
    "plt.ylabel('Vector 2')\n",
    "plt.show()\n",
    "\n",
    "# Plot the cosine similarity for a specific row\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cosine_sim[100], label='Cosine similarity of Vector 100')\n",
    "plt.xlabel('Vector 1')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('Cosine Similarity for BERT Positional Vector 100')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Pre-Training Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is pre-trained using two primary tasks: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**. \n",
    "\n",
    "#### 1. **Masked Language Modeling (MLM)**\n",
    "\n",
    "- In the MLM task, a token is replaced with the special `[MASK]` token and the model then tries to predict the original token based on the surrounding context. \n",
    "- BERT learns to understand the context and fill in the blanks, which helps it develop contextualized embeddings for each word. \n",
    "\n",
    "#### 2. **Next Sentence Prediction (NSP)**\n",
    "\n",
    "- In addition to MLM, BERT is trained to predict whether one sentence logically follows another. This is called the **Next Sentence Prediction (NSP)** task.\n",
    "- BERT takes two sentences (or segments) as input and must determine if the second sentence is a continuation of the first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I want to eat a [MASK].\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_mlm(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "top_k = 5\n",
    "top_k_probabilities, top_k_indices = torch.topk(predictions[0, masked_index, :], top_k, dim=-1)\n",
    "\n",
    "top_k_probabilities = top_k_probabilities.squeeze().tolist()\n",
    "top_k_indices = top_k_indices.squeeze().tolist()\n",
    "\n",
    "top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
    "\n",
    "print(\"\\nMasked Language Modeling:\")\n",
    "print(\"Input tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "print(\"\\nTop 5 Predictions:\")\n",
    "for token, prob in zip(top_k_tokens, top_k_probabilities):\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nsp = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence_a = \"Sherlock Holmes is my favorite detective\"\n",
    "sentence_b = \"I read all the books about him!\"\n",
    "inputs_nsp = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs_nsp['input_ids'][0]))\n",
    "\n",
    "# Get NSP predictions\n",
    "with torch.no_grad():\n",
    "    outputs_nsp = model_nsp(**inputs_nsp)\n",
    "    logits = outputs_nsp.logits\n",
    "    nsp_prediction = torch.argmax(logits).item()\n",
    "\n",
    "nsp_label = \"True\" if nsp_prediction == 0 else \"False\"\n",
    "print(f\"Prediction: {nsp_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_c = \"The stock market saw a significant increase yesterday, surprising many investors.\"\n",
    "inputs_nsp_false = tokenizer(sentence_a, sentence_c, return_tensors='pt')\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs_nsp_false['input_ids'][0]))\n",
    "\n",
    "# Get NSP predictions\n",
    "with torch.no_grad():\n",
    "    outputs_nsp_false = model_nsp(**inputs_nsp_false)\n",
    "    logits_false = outputs_nsp_false.logits\n",
    "    nsp_prediction_false = torch.argmax(logits_false).item()\n",
    "\n",
    "nsp_label_false = \"True\" if nsp_prediction_false == 0 else \"False\"\n",
    "print(f\"Prediction: {nsp_label_false}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism is a crucial component of BERT’s encoder architecture. It allows the model to weigh the importance of each token in a sequence relative to all other tokens from both the left and right sides, making it **bidirectional**.\n",
    "\n",
    "#### 1. **Self-Attention Mechanism**\n",
    "- In BERT, each token in a sequence attends to all other tokens, including itself. This is known as **self-attention**. The goal is to understand the relationship and relevance of each token within the entire sequence.\n",
    "- The attention mechanism allows BERT to dynamically weigh the relevance of each word in the context of the entire sequence. This helps BERT generate contextualized word representations that take into account the entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "\n",
    "sentence = \"The dog ate the food because it was hungry\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model(**inputs)\n",
    "attention = outputs.attentions  # This returns a list of attention matrices\n",
    "\n",
    "# Select the attention from the last layer and the first attention head\n",
    "attention_layer = attention[-1]  # Last layer\n",
    "attention_head = attention_layer[0, 8].detach().numpy()  # First batch and first head\n",
    "\n",
    "# Get the tokens for the sentence\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Plot the attention weights\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(attention_head, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Tokens')\n",
    "plt.title('Self-Attention Weights (Last Layer, Ninth Head)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BERT is pre-trained on general tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), its real power comes from **fine-tuning** on specific tasks. Fine-tuning involves training the pre-trained BERT model on a smaller dataset for a particular NLP task such as sentiment analysis, text classification, named entity recognition, or question answering.\n",
    "\n",
    "The fine-tuning process typically involves:\n",
    "- Adding a task-specific classification head (a simple dense layer) on top of the pre-trained BERT model.\n",
    "- Training the entire model (including both the BERT base and the new classification head) on your labeled dataset.\n",
    "- The fine-tuning process adjusts the weights of the model so that it performs well on the new task, while still retaining the knowledge it gained during pre-training.\n",
    "\n",
    "In this case, we’ll fine-tune BERT for a sentiment classification task. The model will be trained to classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Hugging Face `transformers` library, it is possible to provide a custom function for computing evaluation metrics, such as accuracy, by passing the function to the `Trainer` class.\n",
    "\n",
    "You can pass the `compute_metrics` function as an argument to the `Trainer`. The function will automatically be called during evaluation, and the computed metrics (like accuracy) will be returned as part of the evaluation output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sentiment classification task, we will fine-tune a pre-trained BERT model on the IMDB dataset. The model will be trained to predict whether a movie review is positive or negative based on the text content.\n",
    "\n",
    "In this snippet, the `IMDB` dataset is loaded using the Hugging Face `datasets` library. This dataset contains movie reviews labeled as **positive** or **negative** for binary sentiment classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentiment analysis dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding into a pre-trained BERT model, the text data needs to be tokenized and converted into input features. Let's define a `tokenize_function` that will handle this preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hugging Face's `transformers` library, it is possible to fine-tune pre-trained models using the **Trainer** class, which abstracts much of the complexity of training deep learning models. The **Trainer** is paired with **TrainingArguments**, which define the parameters for training, evaluation, logging, and saving the model’s progress. These components together allow users to easily fine-tune models for downstream tasks with minimal configuration.\n",
    "\n",
    "- **Trainer**: The `Trainer` class provides a high-level interface to train, evaluate, and make predictions using a pre-trained model. It handles tasks like optimization, gradient computation, and model checkpointing.\n",
    "\n",
    "- **TrainingArguments**: This class is used to configure the fine-tuning process. It includes the essential hyperparameters like learning rate, batch size, number of epochs, logging settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_train_epochs = 2\n",
    "\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hugging Face's `transformers` library, after setting up the **Trainer** and **TrainingArguments**, two key methods are used to manage the model’s training and evaluation:\n",
    "\n",
    "- **`trainer.train()`**: This method initiates the fine-tuning process of the pre-trained model. It handles the forward and backward passes, computes gradients, and updates the model's parameters using the specified optimization algorithm. The method processes the data in batches, performs any logging and checkpointing as specified in the `TrainingArguments`, and loops over the dataset for the number of epochs defined. In essence, `train()` automates the entire training lifecycle.\n",
    "\n",
    "- **`trainer.evaluate()`**: After training, this method is used to assess the performance of the model on a validation or test dataset. It runs the model in evaluation mode, meaning no gradients are computed, and it returns metrics such as accuracy, loss, or other custom metrics defined in the `compute_metrics` function. Evaluation is often performed at the end of each epoch during training, but `evaluate()` can also be called manually to test model performance after training.\n",
    "\n",
    "Together, `train()` and `evaluate()` allow for efficient model fine-tuning, performance tracking, and validation, making it easier to adapt pre-trained models to specific tasks with minimal coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "results = trainer.train()\n",
    "print(f\"Accuracy on the train set: {results.metrics['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy on the test set: {results['eval_accuracy']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
