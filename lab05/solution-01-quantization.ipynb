{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantization**\n",
    "\n",
    "Quantization is a process used to reduce the memory requirements and computational complexity of large machine learning models. By representing model parameters with lower-precision values, quantization makes it possible to run models more efficiently on devices with limited memory and computational resources.\n",
    "\n",
    "For large language models (LLMs), quantization can:\n",
    "- **Reduce Memory Usage:** Lower-precision data types (such as int8) use less memory than higher-precision types (like float32), allowing models to fit into memory-constrained environments.\n",
    "- **Improve Inference Speed:** By using simpler operations on smaller data types, quantization can reduce the time it takes for a model to process inputs and generate outputs.\n",
    "- **Preserve Accuracy:** Quantization is carefully designed to minimize the impact on model accuracy, though a trade-off often exists between precision and efficiency.\n",
    "\n",
    "In this lab, we will explore and compare two common types of quantization: **dynamic quantization** and **static quantization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / 1e6  # size in MB\n",
    "    os.remove(\"temp.pth\")\n",
    "    return size\n",
    "\n",
    "print(f\"Model size before quantization: {get_model_size(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The secret of life is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_output = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "baseline_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBaseline model output:\", baseline_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic quantization applies lower precision to model weights and activations at runtime. This method doesnâ€™t require modifications to the model architecture or retraining, which makes it relatively easy to apply.\n",
    "\n",
    "- **Advantages:** \n",
    "  - Quick to implement with minimal changes. No calibration step is needed.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Activations are not pre-quantized, meaning some precision is maintained but at the cost of slightly higher resource use at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import prepare, convert, get_default_qconfig\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to('cpu')\n",
    "\n",
    "# Model size after quantization\n",
    "print(f\"Model size after quantization: {get_model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nQuantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.46.0/quantization/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuantoConfig\n",
    "\n",
    "quantization_config = QuantoConfig(weights=\"int8\",  modules_to_not_convert=[\"lm_head\"])\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "\n",
    "print(f\"Model size after quantization: {get_model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nquantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static quantization, on the other hand, involves converting both weights and activations to lower-precision values before running the model. This is achieved by calibrating the model on a small subset of data to determine appropriate quantization parameters. \n",
    "\n",
    "- **Advantages:** \n",
    "  - Provides greater memory savings than dynamic quantization.\n",
    "  - Can speed up inference more effectively since activations are pre-quantized.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Requires a calibration dataset for accurate quantization, which adds an extra step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTQuantizer, ORTModelForCausalLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoCalibrationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\"\n",
    "\n",
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_id, export=True, use_cache=False, use_io_binding=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantizer = ORTQuantizer.from_pretrained(quantized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = AutoQuantizationConfig.avx512(is_static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing function for the calibration dataset, ensuring consistent shapes with batch size\n",
    "def preprocess(ex, tokenizer):\n",
    "    print(ex.keys())\n",
    "    tokenized_output = tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "    # for each element in the tokenized_output, add a \"position_ids\" key where it is just a range from 0 to the length of the input_ids - 1 \n",
    "    position_ids = []\n",
    "    for input_ids in tokenized_output[\"input_ids\"]:\n",
    "        position_ids.append(list(range(len(input_ids))))\n",
    "    tokenized_output[\"position_ids\"] = position_ids\n",
    "    return tokenized_output\n",
    "\n",
    "# Carica il dataset di calibrazione\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"Salesforce/wikitext\",\n",
    "    dataset_config_name=\"wikitext-2-raw-v1\",\n",
    "    preprocess_function=partial(preprocess, tokenizer=tokenizer),\n",
    "    num_samples=50,\n",
    "    dataset_split=\"test\",\n",
    ")\n",
    "\n",
    "\n",
    "calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "ranges = quantizer.fit(\n",
    "    dataset=calibration_dataset,\n",
    "    calibration_config=calibration_config,\n",
    "    operators_to_quantize=qconfig.operators_to_quantize,\n",
    ")\n",
    "\n",
    "# Apply static quantization on the model\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=\"static_quantized_model\",\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_quantized_path = \"static_quantized_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_quantized_path, use_cache=False, use_io_binding=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret of life is just a in\n",
      "39, that by great the, if the, in and (Piss, \". n the from the very as a the to to very. a \" in, with. the a. the the the I\n",
      "; our and it- the the to the, in. and\n",
      "â€‹, for to and/- that the the the in is as, all a in or- a S d- R- all A- and ( for the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"The secret of life is\"\n",
    "\n",
    "output = pipe(text, max_length=100, do_sample=True, truncation=True)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
